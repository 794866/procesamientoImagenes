{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d533710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow.keras\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras import Input\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization, LeakyReLU, Activation\n",
    "\n",
    "from keras.constraints import maxnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b044e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToImages = '/home/nbellorin/PycharmProjects/procesamientoImagenes/spyderScripts/dataGenerated/resized'\n",
    "\n",
    "#pathToImages = '/home/nbellorin/Documentos/universidad/TFM/neuronalNetworks/ClasificacionImagenesDeportes/sportimages'\n",
    "\n",
    "dirname = os.path.join(os.getcwd(), pathToImages)\n",
    "imgpath = dirname + os.sep \n",
    " \n",
    "images = []\n",
    "directories = []\n",
    "dircount = []\n",
    "prevRoot=''\n",
    "cant=0\n",
    " \n",
    "print(\"leyendo imagenes de \",imgpath)\n",
    " \n",
    "for root, dirnames, filenames in os.walk(imgpath):\n",
    "    for filename in filenames:\n",
    "        if re.search(\"\\.(jpg|jpeg|png|bmp|tiff)$\", filename):\n",
    "            cant=cant+1\n",
    "            filepath = os.path.join(root, filename)\n",
    "            image = plt.imread(filepath)\n",
    "            images.append(image)\n",
    "            b = \"Leyendo...\" + str(cant)\n",
    "            print (b, end=\"\\r\")\n",
    "            if prevRoot !=root:\n",
    "                print(root, cant)\n",
    "                prevRoot=root\n",
    "                directories.append(root)\n",
    "                dircount.append(cant)\n",
    "                cant=0\n",
    "dircount.append(cant)\n",
    "\n",
    "dircount = dircount[1:]\n",
    "dircount[0]=dircount[0]+1\n",
    "print('Directorios leidos:',len(directories))\n",
    "print(\"Imagenes en cada directorio\", dircount)\n",
    "print('suma Total de imagenes en subdirs:',sum(dircount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6481c0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[]\n",
    "indice=0\n",
    "for cantidad in dircount:\n",
    "    for i in range(cantidad):\n",
    "        labels.append(indice)\n",
    "    indice=indice+1\n",
    "print(\"Cantidad etiquetas creadas: \",len(labels))\n",
    " \n",
    "deportes=[]\n",
    "indice=0\n",
    "for directorio in directories:\n",
    "    name = directorio.split(os.sep)\n",
    "    print(indice , name[len(name)-1])\n",
    "    deportes.append(name[len(name)-1])\n",
    "    indice=indice+1\n",
    " \n",
    "y = np.array(labels)\n",
    "X = np.array(images, dtype=np.uint8) #convierto de lista a numpy\n",
    " \n",
    "# Find the unique numbers from the train labels\n",
    "classes = np.unique(y)\n",
    "nClasses = len(classes)\n",
    "print('Total number of outputs : ', nClasses)\n",
    "print('Output classes : ', classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1768eba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mezclar todo y crear los grupos de entrenamiento y testing\n",
    "train_X,test_X,train_Y,test_Y = train_test_split(X,y,test_size=0.2)\n",
    "print('Training data shape : ', train_X.shape, train_Y.shape)\n",
    "print('Testing data shape : ', test_X.shape, test_Y.shape)\n",
    " \n",
    "train_X = train_X.astype('float32')\n",
    "test_X = test_X.astype('float32')\n",
    "train_X = train_X / 255.\n",
    "test_X = test_X / 255.\n",
    " \n",
    "# Change the labels from categorical to one-hot encoding\n",
    "train_Y_one_hot = to_categorical(train_Y)\n",
    "test_Y_one_hot = to_categorical(test_Y)\n",
    " \n",
    "# Display the change for category label using one-hot encoding\n",
    "print('Original label:', train_Y[0])\n",
    "print('After conversion to one-hot:', train_Y_one_hot[0])\n",
    " \n",
    "train_X,valid_X,train_label,valid_label = train_test_split(train_X, train_Y_one_hot, test_size=0.2, random_state=13)\n",
    " \n",
    "print(train_X.shape,valid_X.shape,train_label.shape,valid_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a8e7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNA SOLA CAPA SIN NORMALIZAR\n",
    "INIT_LR = 1e-3\n",
    "epochs = 25\n",
    "batch_size = 100\n",
    " \n",
    "sport_model = Sequential()\n",
    "sport_model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',padding='same',input_shape=(21,28,3)))\n",
    "sport_model.add(LeakyReLU(alpha=0.1))\n",
    "sport_model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "sport_model.add(Dropout(0.5))\n",
    " \n",
    "sport_model.add(Flatten())\n",
    "sport_model.add(Dense(32, activation='linear'))\n",
    "sport_model.add(LeakyReLU(alpha=0.1))\n",
    "sport_model.add(Dropout(0.5)) \n",
    "sport_model.add(Dense(nClasses, activation='softmax'))\n",
    " \n",
    "sport_model.summary()\n",
    " \n",
    "sport_model.compile(loss=tensorflow.keras.losses.categorical_crossentropy, optimizer=tensorflow.keras.optimizers.Adagrad(lr=INIT_LR, decay=INIT_LR / 100),metrics=['accuracy'])\n",
    "\n",
    "sport_train_dropout = sport_model.fit(train_X, train_label, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(valid_X, valid_label))\n",
    " \n",
    "# guardamos la red, para reutilizarla en el futuro, sin tener que volver a entrenar\n",
    "#sport_model.save(\"sports_mnist.h5py\")\n",
    "\n",
    "test_eval = sport_model.evaluate(test_X, test_Y_one_hot, verbose=1)\n",
    " \n",
    "print('Test loss:', test_eval[0])\n",
    "print('Test accuracy:', test_eval[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae1807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNA SOLA CAPA NORMALIZANDO -- OPTIMO\n",
    "INIT_LR = 1e-3\n",
    "epochs = 25\n",
    "batch_size = 100\n",
    " \n",
    "sport_model = Sequential()\n",
    "sport_model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',padding='same',input_shape=(21,28,3)))\n",
    "sport_model.add(LeakyReLU(alpha=0.1))\n",
    "sport_model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "sport_model.add(Dropout(0.5))\n",
    "sport_model.add(BatchNormalization())\n",
    " \n",
    "sport_model.add(Flatten())\n",
    "sport_model.add(Dense(32, activation='linear'))\n",
    "sport_model.add(LeakyReLU(alpha=0.1))\n",
    "sport_model.add(Dropout(0.5)) \n",
    "sport_model.add(Dense(nClasses, activation='softmax'))\n",
    " \n",
    "sport_model.summary()\n",
    " \n",
    "sport_model.compile(loss=tensorflow.keras.losses.categorical_crossentropy, optimizer=tensorflow.keras.optimizers.Adagrad(lr=INIT_LR, decay=INIT_LR / 100),metrics=['accuracy'])\n",
    "\n",
    "sport_train_dropout = sport_model.fit(train_X, train_label, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(valid_X, valid_label))\n",
    " \n",
    "# guardamos la red, para reutilizarla en el futuro, sin tener que volver a entrenar\n",
    "#sport_model.save(\"sports_mnist.h5py\")\n",
    "\n",
    "\n",
    "test_eval = sport_model.evaluate(test_X, test_Y_one_hot, verbose=1)\n",
    " \n",
    "print('Test loss:', test_eval[0])\n",
    "print('Test accuracy:', test_eval[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcef437f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DOS CAPAS NORMALIZANDO\n",
    "INIT_LR = 1e-3\n",
    "epochs = 25\n",
    "batch_size = 100\n",
    " \n",
    "sport_model = Sequential()\n",
    "sport_model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',padding='same',input_shape=(21,28,3)))\n",
    "sport_model.add(LeakyReLU(alpha=0.1))\n",
    "sport_model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "sport_model.add(Dropout(0.5))\n",
    "sport_model.add(BatchNormalization())\n",
    "\n",
    "sport_model = Sequential()\n",
    "sport_model.add(Conv2D(64, kernel_size=(3, 3),activation='linear',padding='same',input_shape=(21,28,3)))\n",
    "sport_model.add(LeakyReLU(alpha=0.1))\n",
    "sport_model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "sport_model.add(Dropout(0.5))\n",
    "sport_model.add(BatchNormalization())\n",
    " \n",
    "sport_model.add(Flatten())\n",
    "sport_model.add(Dense(32, kernel_constraint=maxnorm(3)))\n",
    "sport_model.add(LeakyReLU(alpha=0.1))\n",
    "sport_model.add(Dropout(0.5)) \n",
    "sport_model.add(Dense(nClasses, activation='softmax'))\n",
    " \n",
    "sport_model.summary()\n",
    " \n",
    "sport_model.compile(loss=tensorflow.keras.losses.categorical_crossentropy, optimizer=tensorflow.keras.optimizers.Adagrad(lr=INIT_LR, decay=INIT_LR / 100),metrics=['accuracy'])\n",
    "\n",
    "sport_train_dropout = sport_model.fit(train_X, train_label, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(valid_X, valid_label))\n",
    " \n",
    "# guardamos la red, para reutilizarla en el futuro, sin tener que volver a entrenar\n",
    "#sport_model.save(\"sports_mnist.h5py\")\n",
    "\n",
    "test_eval = sport_model.evaluate(test_X, test_Y_one_hot, verbose=1)\n",
    " \n",
    "print('Test loss:', test_eval[0])\n",
    "print('Test accuracy:', test_eval[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677cab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DOS CAPAS NORMALIZANDO\n",
    "INIT_LR = 1e-3\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    " \n",
    "sport_model = Sequential()\n",
    "sport_model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',padding='same',input_shape=(21,28,3)))\n",
    "sport_model.add(LeakyReLU(alpha=0.1))\n",
    "sport_model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "sport_model.add(Dropout(0.5))\n",
    "sport_model.add(BatchNormalization())\n",
    "\n",
    "sport_model = Sequential()\n",
    "sport_model.add(Conv2D(64, kernel_size=(3, 3),activation='linear',padding='same',input_shape=(21,28,3)))\n",
    "sport_model.add(LeakyReLU(alpha=0.1))\n",
    "sport_model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "sport_model.add(Dropout(0.5))\n",
    "sport_model.add(BatchNormalization())\n",
    " \n",
    "sport_model.add(Flatten())\n",
    "sport_model.add(Dense(32, kernel_constraint=maxnorm(3)))\n",
    "sport_model.add(LeakyReLU(alpha=0.1))\n",
    "sport_model.add(Dropout(0.5)) \n",
    "sport_model.add(Dense(nClasses, activation='softmax'))\n",
    " \n",
    "sport_model.summary()\n",
    " \n",
    "sport_model.compile(loss=tensorflow.keras.losses.categorical_crossentropy, optimizer=tensorflow.keras.optimizers.Adagrad(lr=INIT_LR, decay=INIT_LR / 100),metrics=['accuracy'])\n",
    "\n",
    "sport_train_dropout = sport_model.fit(train_X, train_label, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(valid_X, valid_label))\n",
    " \n",
    "# guardamos la red, para reutilizarla en el futuro, sin tener que volver a entrenar\n",
    "#sport_model.save(\"sports_mnist.h5py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4155df39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MUCHAS CAPAS NORMALIZANDO\n",
    "INIT_LR = 1e-3\n",
    "epochs = 25\n",
    "batch_size = 100\n",
    " \n",
    "sport_model = Sequential()\n",
    "sport_model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',padding='same',input_shape=(21,28,3)))\n",
    "sport_model.add(LeakyReLU(alpha=0.1))\n",
    "sport_model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "sport_model.add(Dropout(0.5))\n",
    "sport_model.add(BatchNormalization())\n",
    "\n",
    "sport_model = Sequential()\n",
    "sport_model.add(Conv2D(64, kernel_size=(3, 3),activation='linear',padding='same',input_shape=(21,28,3)))\n",
    "sport_model.add(LeakyReLU(alpha=0.1))\n",
    "sport_model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "sport_model.add(Dropout(0.5))\n",
    "sport_model.add(BatchNormalization())\n",
    "\n",
    "sport_model = Sequential()\n",
    "sport_model.add(Conv2D(128, kernel_size=(3, 3),activation='linear',padding='same',input_shape=(21,28,3)))\n",
    "sport_model.add(LeakyReLU(alpha=0.1))\n",
    "sport_model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "sport_model.add(Dropout(0.5))\n",
    "sport_model.add(BatchNormalization())\n",
    "\n",
    "sport_model = Sequential()\n",
    "sport_model.add(Conv2D(256, kernel_size=(3, 3),activation='linear',padding='same',input_shape=(21,28,3)))\n",
    "sport_model.add(LeakyReLU(alpha=0.1))\n",
    "sport_model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "sport_model.add(Dropout(0.5))\n",
    "sport_model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "sport_model.add(Flatten())\n",
    "sport_model.add(Dense(64, kernel_constraint=maxnorm(3)))\n",
    "sport_model.add(LeakyReLU(alpha=0.1))\n",
    "sport_model.add(Dropout(0.5)) \n",
    "sport_model.add(Dense(nClasses, activation='softmax'))\n",
    " \n",
    "sport_model.summary()\n",
    " \n",
    "sport_model.compile(loss=tensorflow.keras.losses.categorical_crossentropy, optimizer=tensorflow.keras.optimizers.Adagrad(lr=INIT_LR, decay=INIT_LR / 100),metrics=['accuracy'])\n",
    "\n",
    "sport_train_dropout = sport_model.fit(train_X, train_label, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(valid_X, valid_label))\n",
    " \n",
    "# guardamos la red, para reutilizarla en el futuro, sin tener que volver a entrenar\n",
    "#sport_model.save(\"sports_mnist.h5py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738204f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eval = sport_model.evaluate(test_X, test_Y_one_hot, verbose=1)\n",
    " \n",
    "print('Test loss:', test_eval[0])\n",
    "print('Test accuracy:', test_eval[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3ee6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#========> MODELO 2\n",
    "INIT_LR = 1e-3\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    " \n",
    "sport_model = Sequential()\n",
    "sport_model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',padding='same',input_shape=(21,28,3)))\n",
    "sport_model.add(Activation('relu'))\n",
    "sport_model.add(Dropout(0.2))\n",
    "sport_model.add(BatchNormalization())\n",
    "\n",
    "sport_model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "sport_model.add(Activation('relu'))\n",
    "sport_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "sport_model.add(Dropout(0.2))\n",
    "sport_model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "\n",
    "sport_model.add(Flatten())\n",
    "sport_model.add(Dense(32, activation='linear'))\n",
    "sport_model.add(LeakyReLU(alpha=0.1))\n",
    "sport_model.add(Dropout(0.5)) \n",
    "sport_model.add(Dense(nClasses, activation='softmax'))\n",
    "\n",
    "sport_model.add(Dense(nClasses, activation='softmax'))\n",
    " \n",
    "sport_model.summary()\n",
    " \n",
    "sport_model.compile(loss=tensorflow.keras.losses.categorical_crossentropy, optimizer=tensorflow.keras.optimizers.Adagrad(lr=INIT_LR, decay=INIT_LR / 100),metrics=['accuracy'])\n",
    "\n",
    "sport_train_dropout = sport_model.fit(train_X, train_label, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(valid_X, valid_label))\n",
    " \n",
    "# guardamos la red, para reutilizarla en el futuro, sin tener que volver a entrenar\n",
    "#sport_model.save(\"sports_mnist.h5py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e2a0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eval = sport_model.evaluate(test_X, test_Y_one_hot, verbose=1)\n",
    " \n",
    "print('Test loss:', test_eval[0])\n",
    "print('Test accuracy:', test_eval[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3233e71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
